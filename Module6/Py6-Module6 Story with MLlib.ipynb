{"cells":[{"cell_type":"markdown","source":["#Evaluating Risk for Loan Approvals\n\n## using GLM prediction model\n\n## Business Value\n\nBeing able to accurately assess the risk of a loan application can save a lender the cost of holding too many risky assets. Rather than a credit score or credit history which tracks how reliable borrowers are, we will generate a score of how profitable a loan will be compared to other loans in the past. The combination of credit scores, credit history, and profitability score will help increase the bottom line for financial institution.\n\nHaving a interporable model that a loan officer can use before performing a full underwriting can provide immediate estimate and response for the borrower and a informative view for the lender.\n\nIn this notebook we're playing the role of a Data Scientist and building an elementary model which our Dev Ops team will publish.\n\nData is again Loan dataset - free and publicaly available. More: https://www.kaggle.com/wendykan/lending-club-loan-data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86990dbc-72e8-4160-a623-da29b71d73fa"}}},{"cell_type":"markdown","source":["-sandbox\n### Tracking Experiments with MLflow\n\nOver the course of the machine learning lifecycle, data scientists test many different models from various libraries with different hyperparemeters.  Tracking these various results poses an organizational challenge.  In brief, storing experiements, results, models, supplementary artifacts, and code creates significant challenges in the machine learning lifecycle.\n\nMLflow Tracking is a logging API specific for machine learning and agnostic to libraries and environments that do the training.  It is organized around the concept of **runs**, which are executions of data science code.  Runs are aggregated into **experiments** where many runs can be a part of a given experiment and an MLflow server can host many experiments.\n\nEach run can record the following information:\n\n- **Parameters:** Key-value pairs of input parameters such as the number of trees in a random forest model\n- **Metrics:** Evaluation metrics such as RMSE or Area Under the ROC Curve\n- **Artifacts:** Arbitrary output files in any format.  This can include images, pickled models, and data files\n- **Source:** The code that originally ran the experiement\n\nMLflow tracking also serves as a **model registry** so tracked models can easily be stored and, as necessary, deployed into production.\n\nExperiments can be tracked using libraries in Python, R, and Java as well as by using the CLI and REST calls.\n\n<div><img src=\"https://pages.databricks.com/rs/094-YMS-629/images/mlflow-tracking.png\" style=\"height: 300px; margin: 20px\"/></div>\n<div><img src=\"https://pages.databricks.com/rs/094-YMS-629/images/3 - Unify data and ML across the full lifecycle.png\" width=\"950\"></div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3ddff9b-36d1-4535-ad85-a54f3a64478a"}}},{"cell_type":"markdown","source":["### Databricks MLflow Integration\nManaged MLFlow tracking server is available in community edition, the model registry is not.\nThis notebook will walk you through creating a model, track with MLFlow and **show** you how to register it with the registry when not in Community Edition\n\nAutomated MLFlow tracking for Hyper-parameter tuning demo:\n- Where we use features in this notebook that aren't available in community edition, we'll just leave the markdown or comment them out\n- here we've seperated out the deployment of the model to AWS, but a data scientist could deploy the model if they had the correct access to AWS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e873051e-84e3-450e-b002-02bd392c96eb"}}},{"cell_type":"code","source":["# Imports\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n\nimport mlflow,pyspark\nimport mlflow.mleap\nimport os\n\nmlf_ver = mlflow.__version__\nmlf_max_ver = (1,11,0)\n\nmlf_ver_i = tuple(map(int, mlf_ver.split(\".\")))\n\n#assert mlf_ver_i <= mlf_max_ver, f\"Current MLFlow Version {mlf_ver}. Must at most {mlf_max_ver}. Please use a cluster with max runtime 6.3ML\"\n\nmlflow.pyspark.ml.autolog()\n\nprint(f\"MLFlow Version:{mlflow.__version__}\\n\" +\\\n      f\"Pyspark Version:{pyspark.__version__}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Ensure we're running at the correct runtime","showTitle":true,"inputWidgets":{},"nuid":"8983c974-41d1-4e4c-b644-b7d1f1e3f92f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Data Engineering\n- clean up the features for modelling\n- review the feature characteristics\n- add additional features we may wish to include in our model\n- choose your databsename,deltatable location path ,registeredmodelname uniquely with a Thumbrule your firstname followed by (four random digits)zzzz_ml_devday example : aws_sko  do  a find replace using the edit menu of the notebook above find and replace naseer0814 to your desired uniquename using the thumb rule above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90e4741c-7b64-4975-99a9-d2acf554d6e2"}}},{"cell_type":"code","source":["%sql \nDROP DATABASE IF EXISTS aws_ml_devday CASCADE;\nCREATE DATABASE IF NOT EXISTS aws_ml_devday;\nUSE aws_ml_devday;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"205d016c-a073-4f5d-917e-e491e7701f45"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE DATABASE aws_ml_devday;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35e877ef-5e02-4431-be07-30820176d452"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Configure location of loanstats_2012_2017.parquet\nlspq_path = \"/databricks-datasets/samples/lending_club/parquet/\"\n\n# Read loanstats_2012_2017.parquet\ndata = spark.read.parquet(lspq_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Import Data","showTitle":true,"inputWidgets":{},"nuid":"8d77abc4-4d6f-4df1-9994-b7a939ddd9b4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/databricks/koalas/master/icons/koalas-logo.png\" width=150/>\n\nCommonly used by data scientists, pandas is a Python package that provides easy-to-use data structures and data analysis tools for the Python programming language. However, pandas does not scale out to big data. Koalas fills this gap by providing pandas equivalent APIs that work on Apache Spark. Koalas is useful not only for pandas users but also PySpark users, because Koalas supports many tasks that are difficult to do with PySpark, for example plotting data directly from a PySpark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8028e36-dad0-4020-8b66-0cd5d5083c08"}}},{"cell_type":"code","source":["import databricks.koalas as ks\nkdf = data.to_koalas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c1c4838-0233-4d26-bc2f-ad903ddef355"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["kdf[['loan_amnt','loan_status']].head(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"We can now leverage familiar Pandas syntax for exploration...","showTitle":true,"inputWidgets":{},"nuid":"7027b8a8-4368-44f8-8a74-3eb724c81974"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["kdf.groupby(['loan_status']).agg({'loan_amnt': ['sum','count']})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b136214-1fc4-49e4-a453-83433e2e9a84"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import seaborn as sns\nsns.set_style('whitegrid')\n\n%matplotlib inline\nkdf['bad_loan'] = (kdf['loan_status'] != 'Fully Paid').astype('int')\n\nsns.barplot(data=\n  (kdf[(~kdf.grade.isnull()) & (kdf.loan_status.isin([\"Default\", \"Charged Off\", \"Fully Paid\"]))]\n .pivot_table(index=['grade'],columns='bad_loan',values='loan_amnt',aggfunc='count')\n .assign(df_ratio=lambda x: x[1]/(x[0]+x[1]))\n .sort_index()\n .reset_index()\n .to_pandas()\n),\n  x='grade',y='df_ratio'\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"... and plotting","showTitle":true,"inputWidgets":{},"nuid":"e1f0caa6-01e9-4df8-9637-066bc0bda550"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nimport pandas as pd\n\ndef clean_pct(x: pd.Series) -> pd.Series:\n  return x.str.strip('%').astype('double')\n\npandas_udf(clean_pct,returnType=FloatType())\n\ndef clean_loan_stats(df):\n  kdf = df.to_koalas()\n  kdf = kdf[[\"loan_status\", \"int_rate\", \"revol_util\", \"issue_d\", \"earliest_cr_line\", \"emp_length\", \"verification_status\", \"total_pymnt\", \"loan_amnt\", \"grade\", \"annual_inc\", \"dti\", \"addr_state\", \"term\", \"home_ownership\", \"purpose\", \"application_type\", \"delinq_2yrs\", \"total_acc\"]]\n  \n  kdf = kdf[kdf.loan_status.isin([\"Default\", \"Charged Off\", \"Fully Paid\"])]\n  \n  # Create bad loan label, this will include charged off, defaulted, and late repayments on loans...\n  kdf[\"bad_loan\"] = (kdf['loan_status'] != 'Fully Paid').astype('int')\n  \n  # Clean and convert some of the string data into numerical data\n  for c in ['int_rate', 'revol_util']:\n    kdf[c] = clean_pct(kdf[c])\n  \n  kdf['issue_year'] = kdf['issue_d'].str.slice(4,9).astype('double') \n  kdf['earliest_year'] = kdf['earliest_cr_line'].str.slice(4,9).astype('double')\n  kdf['total_pymnt'] = kdf['total_pymnt'].astype('float')\n  \n  kdf[\"credit_length_in_years\"] = kdf['issue_year'] - kdf['earliest_year']\n  \n  # Converting emp_length column into numeric...\n  kdf['emp_length'] = (kdf['emp_length']\n                       .str.replace(\"([ ]*[a-zA-Z].*)|(n\\/a)\", \"\")\n                       .str.replace(\"< 1\", \"0\")\n                       .str.replace(\"10\\\\+\", \"10\")\n                       .astype('float')\n                      )\n  \n  return kdf.to_spark()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Using Koalas, we create a pipeline to prepare our data","showTitle":true,"inputWidgets":{},"nuid":"ec2f7a8e-558c-4ee9-9c01-66fd21671e0d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["loan_stats = clean_loan_stats(data)\ndisplay(loan_stats)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Display our engineered columns/features - note the cleaned up numerics and categoricals","showTitle":true,"inputWidgets":{},"nuid":"20df02b7-b90c-46aa-a4d5-36e76a29c8bb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Leverage Databricks Delta for Data versioning\nWe leverage Delta Lake for storing the cleaned data. Delta Lake comes with a number of very handy features in the context of Machine Learning: For example, ACID transactions allow us to merge merge new data into the data set while maintaining a consistent state and time-travel allows us to restore previous versions of a table that may have been used to train a model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d1089e5-67be-4b68-9c9a-21e8ac74fea0"}}},{"cell_type":"code","source":["\n# Configure Path and temp view name\nDELTALAKE_GOLD_PATH = ('/ml/aws_sko/loan_stats.delta')\n\n#tviewname = dbutils.widgets.get('tviewname')\n\n# Remove table if it exists\ndbutils.fs.rm(DELTALAKE_GOLD_PATH, recurse=True)\n\n# Save table as Delta Lake\n(loan_stats\n .write\n .format(\"delta\")\n .option('path',DELTALAKE_GOLD_PATH)\n .mode(\"overwrite\")\n .saveAsTable('loan_stats_delta')\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f752dc6-1a20-4a2e-a137-e7e792ce53e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nDESCRIBE TABLE EXTENDED loan_stats_delta;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44cc9579-20b3-4e12-8871-94e19da2a5f9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from loan_stats_delta;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3389bd00-4447-4463-b099-ecdf7bb84e6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(dbutils.fs.ls(DELTALAKE_GOLD_PATH))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a92ffc86-e0e7-4f98-8f83-16608a8c04f3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect issue_year, count(*) \nfrom loan_stats_delta\ngroup by 1\norder by 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Let us pretend someone accidentally deletes a chunk of our data","showTitle":true,"inputWidgets":{},"nuid":"b855e403-89b1-439c-b394-4a920b025c97"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\ndelete\nfrom loan_stats_delta\nwhere issue_year <= 2013"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3156540e-1771-4baa-8b5f-040c141b6855"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect issue_year, count(*) \nfrom loan_stats_delta\ngroup by 1\norder by 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27be4c9c-0e84-4411-af84-64fd48bbda12"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\ndescribe history loan_stats_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Let us have a look at the version history of the table","showTitle":true,"inputWidgets":{},"nuid":"65b4b8e1-f87a-4625-8571-a38f6bd8afd8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect issue_year, count(*) \nfrom loan_stats_delta@v0\ngroup by 1\norder by 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Luckily, time-travel can bring our data back","showTitle":true,"inputWidgets":{},"nuid":"3d589b36-5c42-45b8-83fe-e3cfa867a11b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nRESTORE TABLE loan_stats_delta TO VERSION AS OF 0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"And we can restore our data to the complete state","showTitle":true,"inputWidgets":{},"nuid":"99a2849c-931b-4828-8c78-7c9fc44feffd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect *\nfrom loan_stats_delta\nwhere issue_year <= 2013\nlimit 20"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d39fb32d-802b-4e42-ac4b-efdf7ab86e27"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Schema Evolution\nWith the `mergeSchema` option, you can evolve your Delta Lake table schema. Imagine that we decide to add two additional features:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02af1175-a219-41cf-b2ad-1eab8ef4c749"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Map multiple levels into one factor level for verification_status...\")\nloan_stats = loan_stats.withColumn('verification_status', trim(regexp_replace(loan_stats.verification_status, 'Source Verified', 'Verified')))\n\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Calculate the total amount of money earned or lost per loan...\")\nloan_stats = loan_stats.withColumn('net', round( loan_stats.total_pymnt - loan_stats.loan_amnt, 2))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Adding two additional features, so updates the schema of the table","showTitle":true,"inputWidgets":{},"nuid":"8edf60b2-d559-4a7f-9c5c-237a29005a79"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Add the mergeSchema option\nloan_stats.write.option(\"mergeSchema\",\"true\").format(\"delta\").mode(\"overwrite\").save(DELTALAKE_GOLD_PATH)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Now intentionally overwrite the schema","showTitle":true,"inputWidgets":{},"nuid":"8703215a-bf17-4263-a7e6-f618218faf42"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql \nselect * from loan_stats_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ec15dcd-2705-40f9-8cd3-9a7140df87b4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# note the positively skewed distribution\ndisplay(loan_stats)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Distribution and Correlations","showTitle":true,"inputWidgets":{},"nuid":"ba10ec4a-fcfe-4438-a889-029dfa885cb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# First iteration over the model\n\nAcknowledging our data skew, we're ready to take a look at our first model\nLogistic Regression is a staple in classification (although assumes a normal distribution - we're making several leaps here in the interest of showing the tooling)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99541001-adc6-42f7-99fe-79faa013b552"}}},{"cell_type":"code","source":["# again, for expediancy, we're splitting on date - and caching\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Setting variables to predict bad loans\")\nmyY = \"bad_loan\"\ncategoricals = [\"term\", \"home_ownership\", \"purpose\", \"addr_state\",\n                \"verification_status\",\"application_type\"]\nnumerics = [\"loan_amnt\",\"emp_length\", \"annual_inc\",\"dti\",\n            \"delinq_2yrs\",\"revol_util\",\"total_acc\",\n            \"credit_length_in_years\"]\nmyX = categoricals + numerics\n\nloan_stats2 = loan_stats.select(myX + [myY, \"int_rate\", \"net\", \"issue_year\"])\n\n# note the use of cache here: if using a delta optimised instance this isn't required\n# we'll accept that splitting on date isn't best practice\ntrain = loan_stats2.filter(loan_stats2.issue_year <= 2015)\nvalid = loan_stats2.filter(loan_stats2.issue_year > 2015)\n\ntrain.count(), valid.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Set Response and Predictor Variables","showTitle":true,"inputWidgets":{},"nuid":"24b88f92-38be-483e-bbe9-a548ad6e26af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(train) # intrigingly the distribution is less skewed\n# display(valid) # seems more positively skewed"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86a3e170-50b4-42a0-9a83-7b5257a1f475"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Logistic Regression Notes\n* We will be using the Apache Spark pre-installed GLM and GBTClassifier models in this noteboook\n* **GLM** is in reference to *generalized linear models*; the Apache Spark *logistic regression* model is a special case of a [generalized linear model](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#logistic-regression)\n* We will also use BinaryClassificationEvaluator, CrossValidator, and ParamGridBuilder to tune our models.\n* References to max F1 threshold (i.e. F_1 score or F-score or F-measure) is the measure of our logistic regression model's accuracy; more information can be found at [F1 score](https://en.wikipedia.org/wiki/F1_score).\n* **GBTClassifier** is in reference to *gradient boosted tree classifier* which is a popular classification and regression method using ensembles of decision trees; more information can be found at [Gradiant Boosted Tree Classifier](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#gradient-boosted-tree-classifier)\n* In a subsequent notebook, we will be using the XGBoost, an optimized distributed gradient boosting library.  \n  * Underneath the covers, we will be using *XGBoost4J-Spark* - a project aiming to seamlessly integrate XGBoost and Apache Spark by fitting XGBoost to Apache Spark’s MLLIB framework.  More inforamtion can be found at [XGBoost4J-Spark Tutorial](https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html).\n  \n### MLFlow\n* we don't need to log the individual paramaters or metrics, with 'trackMLlib.enabled' this a tracked.\n* MLFlow autotracking is also available (see more [here](https://www.mlflow.org/docs/latest/python_api/mlflow.keras.html))\n* results from the next cell will be captured under the 'runs' button"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30ba17b6-feaf-4932-9473-9cdcc163eb8d"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\nfrom pyspark.ml.feature import StandardScaler, Imputer\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n## Current possible ways to handle categoricals in string indexer is 'error', 'keep', and 'skip'\nindexers = map(lambda c: StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid = 'keep'), categoricals)\nohes = map(lambda c: OneHotEncoder(inputCol=c + \"_idx\", outputCol=c+\"_class\"),categoricals)\nimputers = Imputer(inputCols = numerics, outputCols = numerics)\n\n# Establish features columns\nfeatureCols = list(map(lambda c: c+\"_class\", categoricals)) + numerics\n\n# Build the stage for the ML pipeline\n# Build the stage for the ML pipeline\nmodel_matrix_stages = list(indexers) + list(ohes) + [imputers] + \\\n                     [VectorAssembler(inputCols=featureCols, outputCol=\"features\"), StringIndexer(inputCol=\"bad_loan\", outputCol=\"label\")]\n\n# Apply StandardScaler to create scaledFeatures\nscaler = StandardScaler(inputCol=\"features\",\n                        outputCol=\"scaledFeatures\",\n                        withStd=True,\n                        withMean=True)\n\n# Use logistic regression \nlr = LogisticRegression(maxIter=10, elasticNetParam=0.5, featuresCol = \"scaledFeatures\")\n\n# Build our ML pipeline\npipeline = Pipeline(stages=model_matrix_stages+[scaler]+[lr])\n\n# Build the parameter grid for model tuning\n# (building with one parameter to reduce training time on Community Edition)\nparamGrid = ParamGridBuilder() \\\n              .addGrid(lr.regParam, [0.1, 0.01]) \\\n              .build()\n\n# uncomment for the elasticNetParam for a more interesting grid (increases the training time)\n# paramGrid = ParamGridBuilder() \\\n#              .addGrid(lr.regParam, [0.1, 0.01]) \\\n#              .addGrid(lr.elasticNetParam, [0.5, 0.2]) \\\n#              .build()\n\n# Execute CrossValidator for model tuning\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=5)\n\n# this will auto track our params, also inherits the current active run\ncvModel = crossval.fit(train) \n\nglm_model = cvModel.bestModel\n\n# Return ROC\nlr_summary = glm_model.stages[len(glm_model.stages)-1].summary\n\n# you may need to adjust the following to get the FPR on the X axis, and TPR on the Y-Axis\ndisplay(lr_summary.roc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Build Grid of GLM Models w/ Standardization+CrossValidation","showTitle":true,"inputWidgets":{},"nuid":"34a38c19-10f7-4318-ae8b-fa52c06233bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Review the results in the run window\n\nUsing the comparison tool we can now make a choice about our model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20e7d09a-9ef8-4573-9bd3-060e64829265"}}},{"cell_type":"markdown","source":["# MLFlow and Model registry\n* log the model and model registry to catalog\n* this time we want to log the model as part of the run\n* we'll use the UI to explore the model registry from our DS persona"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a964e153-c186-497c-be70-2854605c7d4f"}}},{"cell_type":"code","source":["registered_model_name = 'aws-ml-devday'\n\n# we need a run to attach to and record and log the model:\nwith mlflow.start_run(run_name='production_run'):\n  \n  cvModel = crossval.fit(train) # this will auto track our params, also inherits the current active run\n  \n  glm_model = cvModel.bestModel\n  \n  #from mlflow import spark\n  \n  # log the model and register it with the model repository \n  try:\n    mlflow.spark.log_model(glm_model, artifact_path='glm_model', registered_model_name=registered_model_name) \n  except:\n    mlflow.spark.log_model(glm_model, artifact_path='glm_model')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28f8b0c3-2901-4baa-a45a-e78104628d50"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# note that no metrics are captured\nfrom pyspark.ml.classification import GBTClassifier\n\n# Establish stages for our GBT model\nindexers = map(lambda c: StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid = 'keep'), categoricals)\nimputers = Imputer(inputCols = numerics, outputCols = numerics)\nfeatureCols = list(map(lambda c: c+\"_idx\", categoricals)) + numerics\n\n# Define vector assemblers\nmodel_matrix_stages = list(indexers) + [imputers] + \\\n                     [VectorAssembler(inputCols=featureCols, outputCol=\"features\"), StringIndexer(inputCol=\"bad_loan\", outputCol=\"label\")]\n\n# Define a GBT model.\ngbt = GBTClassifier(featuresCol=\"features\",\n                    labelCol=\"label\",\n                    lossType = \"logistic\",\n                    maxBins = 52,\n                    maxIter=20,\n                    maxDepth=5)\n\n# Chain indexer and GBT in a Pipeline\npipeline = Pipeline(stages=model_matrix_stages+[gbt])\n\n# Train model.  This also runs the indexer.\ngbt_model = pipeline.fit(train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Build GBT Model","showTitle":true,"inputWidgets":{},"nuid":"f2f4484f-007f-4ec3-8a58-bc2cf9000d02"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.ml.linalg import Vectors\n\ndef extract(row):\n  return (row.net,) + tuple(row.probability.toArray().tolist()) +  (row.label,) + (row.prediction,)\n\ndef score(model,data):\n  pred = model.transform(data).select(\"net\", \"probability\", \"label\", \"prediction\")\n  pred = pred.rdd.map(extract).toDF([\"net\", \"p0\", \"p1\", \"label\", \"prediction\"])\n  return pred \n\ndef auc(pred):\n  metric = BinaryClassificationMetrics(pred.select(\"p1\", \"label\").rdd)\n  return metric.areaUnderROC\n\nglm_train = score(glm_model, train)\nglm_valid = score(glm_model, valid)\ngbt_train = score(gbt_model, train)\ngbt_valid = score(gbt_model, valid)\n\nglm_train.createOrReplaceTempView(\"glm_train\")\nglm_valid.createOrReplaceTempView(\"glm_valid\")\ngbt_train.createOrReplaceTempView(\"gbt_train\")\ngbt_valid.createOrReplaceTempView(\"gbt_valid\")\n\n# instead of running this, we'll use MLFlow to bring it all together in a single experiment\n # print (\"GLM Training AUC:\" + str(auc(glm_train)))\n # print (\"GLM Validation AUC :\" + str(auc(glm_valid)))\n # print (\"GBT Training AUC :\" + str(auc(gbt_train)))\n#  print (\"GBT Validation AUC :\" + str(auc(gbt_valid)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Grab Model Metrics","showTitle":true,"inputWidgets":{},"nuid":"2751fa74-35a2-4105-a3d7-5814209ca497"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n// import org.apache.spark.sql.functions.typedLit\nimport org.apache.spark.sql.functions.{array, lit, map, struct}\n\ndef roc(pred:org.apache.spark.sql.DataFrame, model_id:String): org.apache.spark.sql.DataFrame = {\n  var testScoreAndLabel = pred.select(\"p1\", \"label\").map{ case Row(p:Double,l:Double) => (p,l)}\n  val metrics = new BinaryClassificationMetrics(testScoreAndLabel.rdd, 100)\n  val roc = metrics.roc().toDF().withColumn(\"model\", lit(model_id))\n  return roc\n}\n\nval glm_train = roc( spark.table(\"glm_train\"), \"glm_train\")\nval glm_valid = roc( spark.table(\"glm_valid\"), \"glm_valid\")\nval gbt_train = roc( spark.table(\"gbt_train\"), \"gbt_train\")\nval gbt_valid = roc( spark.table(\"gbt_valid\"), \"gbt_valid\")\n\nval roc_curves = glm_train.union(glm_valid).union(gbt_train).union(gbt_valid)\n\n//settings for the chart: Series grouping : model, x-axis: _1, y-axis: _2\ndisplay(roc_curves)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Stacked ROC Curves","showTitle":true,"inputWidgets":{},"nuid":"c586cf87-39ca-460a-9f21-8d189d3421af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Quantify the Business Value\n\nA great way to quickly understand the business value of this model is to use the confusion matrix to evaluate the cost of a poor loan decision.  The definition of our matrix is as follows:\n\n* Prediction=1, Label=1 (Blue) : Correctly found bad loans. sum_net = loss avoided.\n* Prediction=1, Label=0 (Orange) : Incorrectly labeled bad loans. sum_net = profit forfeited.\n* Prediction=0, Label=1 (Green) : Incorrectly labeled good loans. sum_net = loss still incurred.\n* Prediction=0, Label=0 (Red) : Correctly found good loans. sum_net = profit retained.\n\nThe following code snippet calculates the following confusion matrix."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e9f8bad-23b9-4386-b4d1-319ae957b29f"}}},{"cell_type":"code","source":["# configure plot as bar; series groupings: label, prediction, value; sum_net\ndisplay(glm_valid.groupBy(\"label\", \"prediction\").agg((sum(col(\"net\"))).alias(\"sum_net\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98370fd6-f34a-4b36-ab56-17f6a2a46c64"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://pages.databricks.com/rs/094-YMS-629/images/stop.png?raw=true\" width=50/>\n### Stop the notebook for setting experiment location"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a80fc22-ed67-4a57-ad3d-9bee91cbfa3a"}}},{"cell_type":"code","source":["dbutils.notebook.exit(\"stop\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9899995-488b-4dde-a9ea-fd8f6841b072"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# by setting the experiment we now log to a single location in the workspace: In the sidebar, click the home button select your user name and then right click and create a folder named mlflow and set the the experiment below. example : mlflow.set_experiment('/Users/userfirstname.userlastname@emaildomain.com/mlflow/gbt-glm-new')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db4af5b8-72aa-4dd6-8cb5-3e38c31d162e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls('/')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee153bbe-dac0-44ef-9e1f-d7a6c3dee2e4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["user_email=dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\nmlflow.create_experiment(f\"/Users/{user_email}/gbt-glm-new1\")\n\nwith mlflow.start_run(run_name=\"glm\"):\n  mlflow.log_param(\"regParam\", 0.01)\n  mlflow.log_metric(\"auc\", auc(glm_valid))\n  \n  # we can log via the API or through the UI (we've already registered one model)\n  # will not register for community edition\n  try:\n    mlflow.spark.log_model(glm_model, artifact_path='glm_model', registered_model_name=registered_model_name) \n  except:\n    mlflow.spark.log_model(glm_model, 'glm_model')\n    \nwith mlflow.start_run(run_name=\"gbt\"):\n  mlflow.log_param(\"maxDepth\", 5)\n  mlflow.log_metric(\"auc\", auc(gbt_valid))\n  mlflow.spark.log_model(gbt_model, 'gbt_model')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Now bring all the metrics together and register the model again in the model registry","showTitle":true,"inputWidgets":{},"nuid":"c97e569f-dffd-4edc-8c01-19d8e91501b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Py6-Module6 Story with MLlib","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4203640582532554}},"nbformat":4,"nbformat_minor":0}
