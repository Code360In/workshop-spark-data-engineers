{"cells":[{"cell_type":"markdown","source":["# Module 1 - Part 2 & Module 2 (RDD Part)\n\n- SparkContext object\n- RDDs\n- Operations on RDDs:\n    - transformations\n    - actions\n- DataFrames"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8559013f-a992-440e-bf4e-22c1e7da2522"}}},{"cell_type":"markdown","source":["## SparkContext\n\nOnce we create a `SparkContext (sc)` object, we use it for orchestrating the allocated resources. T"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cdc1908-aa3d-4762-89ac-1af78b86bbff"}}},{"cell_type":"code","source":["import sys\nfrom random import random\nfrom pyspark import SparkContext, SparkConf\n\n# Create a SparkConf configuration object which sets the Spark Application name\nconf = SparkConf().setAppName(\"Spark Intro\").setMaster(\"local[1]\") # reserve 1 core\n\n# Create the SparkContext object.\n# In this case we are using `.getOrCreate` method to be able to rerun the same cell multiple times\nsc = SparkContext.getOrCreate(conf=conf) # Alternatively, use SparkContext(conf=conf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca5d3842-68ee-4617-8c18-53b1395df27d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Resilient Distributed Datasets - RDDs\n\nResilient Distributed Dataset (RDD) is the elementary data structure used in Spark. RDDs are immutable and fault-tolerant, meaning that once loaded, the data cannot be changed, and because of that the system is able to recalculate results from failing nodes. RDDs also enable operations on the enclosed data to be executed in parallel on multiple nodes.\n\nThere are 3 main ways of creating RDDs:\n1. From an existing collection - parallelize the existing collection from the target programming language, such as an array.\n2. Transforming an existing RDD - applying transformations on an RDD yields a new RDD.\n3. Loading an external dataset - ability to load data from an existing source (e.g. from a file system)\n\nThere are 2 types of operations that can be applied to RDDs:\n1. Transformations - a lazy operation which yields a new RDD\n2. Actions - return a value to the driver program, i.e. execute all the predefined lazy operations of the RDD\n\nWe are going to explore all of these options in the examples below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4102757e-5c50-49a6-88e6-014b49a3f608"}}},{"cell_type":"markdown","source":["### Creating RDDs"],"metadata":{"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e4470fe-1bb8-4bfd-af41-0e4f57099d55"}}},{"cell_type":"code","source":["intro_rdd = sc.parallelize(range(20), 8)\n\nprint(f\"The assigned ID of the RDD: {intro_rdd.id()}\")\n\nintro_rdd.setName(\"Example RDD\")\nprint(f\"Name of the RDD: {intro_rdd.name()}\")"],"metadata":{"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc7c92ed-bc69-4692-a229-0190266243e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The assigned ID of the RDD: 1\nName of the RDD: Example RDD\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The assigned ID of the RDD: 1\nName of the RDD: Example RDD\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Caching the intermediate results\n# it is useful when loading big thrid party datasets into memory,\n# so you do not have to reaload the data every time you perform an action\nintro_rdd.cache().collect()"],"metadata":{"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c985257-1b9f-4c6c-8246-6ff8a0930a80"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Types of caching\nDepending on where you would like to cache the data, there are multiple types of caching:\n- DISK_ONLY - serialized, on disk\n- MEMORY_ONLY - deserialized, in memory\n- MEMORY_AND_DISK - if not enough memory, spill to disk\n- OFF_HEAP - outside the process' allocated memory (heap), not processed by the garbage collector"],"metadata":{"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0895fc3c-a90d-42c7-ad6e-8e07a9fe6f2a"}}},{"cell_type":"markdown","source":["### RDD partitioning\nPartitions are the main unit of parallelism in Spark. Split the data so that it can be managed in parallel.\nThe partitions are in-memory, so they are not stored in the disk, unless there is no sufficient space.\n\nUsually the default number of partitions is the number of cores on the worker. So each worker gets a single partition to\nperform the computations and return the results to the driver program."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9de2280-46c7-4eba-9e67-3da75a327eec"}}},{"cell_type":"code","source":["print(f\"Number of partitions: {intro_rdd.getNumPartitions()}\") \nprint(f\"Data per partitions: {intro_rdd.glom().collect()}\") # glom takes the data from the partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cb34acd-dfea-4aac-89ee-f97e0bc1817d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of partitions: 8\nData per partitions: [[0, 1], [2, 3, 4], [5, 6], [7, 8, 9], [10, 11], [12, 13, 14], [15, 16], [17, 18, 19]]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of partitions: 8\nData per partitions: [[0, 1], [2, 3, 4], [5, 6], [7, 8, 9], [10, 11], [12, 13, 14], [15, 16], [17, 18, 19]]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Managing the partitions\n\nIt often makes sense to manage the partitions, how many there are and how the data is distributed among partitions.\nWe would like to have an optimal amount of partitions based on the problem that we are trying to solve.\n\nIf there are too many partitions then the scheduler is going to have trouble allocating enough resources for them,\nresulting in spending more time then previously required.\n\nOn the other hand, if there are too few partitions, it can result in lack of parallelism, thus underutilized resources."],"metadata":{"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"232f7e25-baa9-487c-bb0a-10720745279e"}}},{"cell_type":"markdown","source":["### Repartitioning\n\nOne way of managing the partitions is to use `repartition`. This method fully shuffles the partitions and the data,\nand that is why it is a very *expensive* process, involving: data serialization, moving among partitions, data deserialization.\n\nThis method is usually used when increasing the number of partitions, but in this example we are going to demonstrate a\ndecrease in the number of partitions, to illustrate their effect."],"metadata":{"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f5667c4-6b6a-47a8-b8ce-0e5030c33445"}}},{"cell_type":"code","source":["repartitioned_rdd = intro_rdd.repartition(6) # full shuffle\nprint(f\"Number of partitions: {repartitioned_rdd.getNumPartitions()}\") \nprint(f\"Data per partitions: {repartitioned_rdd.glom().collect()}\")"],"metadata":{"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a28b0130-9e37-4f00-998c-37c42d3ac3e7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of partitions: 6\nData per partitions: [[], [0, 1, 12, 13, 14], [7, 8, 9, 10, 11], [], [5, 6, 15, 16, 17, 18, 19], [2, 3, 4]]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of partitions: 6\nData per partitions: [[], [0, 1, 12, 13, 14], [7, 8, 9, 10, 11], [], [5, 6, 15, 16, 17, 18, 19], [2, 3, 4]]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Coalesce\n\nCoalesce is another way of managing the partitions, and it tries avoiding a full shuffle operation.\nIt works in such a way, that it tries not to leave any empty partitions."],"metadata":{"pycharm":{"name":"#%% md\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47931b67-5f18-4923-b597-1605f92c16f3"}}},{"cell_type":"code","source":["coalesced = intro_rdd.coalesce(6)\nprint(f'Number of partitions: {coalesced.getNumPartitions()}')\nprint(f'Data per partitions: {coalesced.glom().collect()}')"],"metadata":{"pycharm":{"name":"#%%\n"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b78f9af-2101-4909-9ce5-6024c9a04d4b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of partitions: 6\nData per partitions: [[0, 1, 17, 18, 19], [2, 3, 4], [7, 8, 9, 15, 16], [10, 11], [5, 6], [12, 13, 14]]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of partitions: 6\nData per partitions: [[0, 1, 17, 18, 19], [2, 3, 4], [7, 8, 9, 15, 16], [10, 11], [5, 6], [12, 13, 14]]\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### RDD operations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ce98026-e7af-4263-8312-2adcaf43fc52"}}},{"cell_type":"code","source":["transformed_rdd = intro_rdd.\\\n                    filter(lambda x: x % 3 == 0).\\\n                    map(lambda x: x + 1)\nprint(f\"Result: {transformed_rdd.collect()}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d2dd107-99e2-4e15-ae4b-87d62fbcf56c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Result: [1, 4, 7, 10, 13, 16, 19]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Result: [1, 4, 7, 10, 13, 16, 19]\n"]}}],"execution_count":0},{"cell_type":"code","source":["sum_rdd = intro_rdd.\\\n                    filter(lambda x: x % 3 == 0).\\\n                    map(lambda x: x + 1).\\\n                    reduce(lambda x, y: x + y)\nprint(f\"Result: {sum_rdd}\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79ba47c3-c317-4a10-860b-b183db57c79d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Result: 70\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Result: 70\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Finally, stop the application.\nsc.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72010bbf-78ff-4d01-ae16-f193091a7149"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72f9d7f8-7ba6-4412-8a15-967c61724d9d"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"DSSpark","language":"python","name":"dsspark"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.3","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"Py1-Module1-part2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4046178255512684}},"nbformat":4,"nbformat_minor":0}
